* DONE Get linear_regression.py to live-plot values of interest
** Average training, testing costs
* DONE Write unit test for ReLU node.
* DONE Write unit test for Softmax node, against pylearn2's.
* DONE Implement CrossEntropy node, unit-test against pylearn2's.
* DONE Implement Dropout
** Requirements:
*** For a given unit, include its inputs with probability p (aka zero out with prob. 1-p).
*** Scale the inputs by 1/p to compensate for lowered output vector magnitude.
*** If the inputs are the output of some parameterized function, the parameters' learning rates should be scaled by p^2 (lowered), to compensate for heightend gradient due to scaling by 1/p above.
** What pylearn2 does
*** dropout is a cost object, with its own rng. Evaluating it causes it to call mlp.dropout_fprop() instead of mlp.fprop().
*** This in turn causes it to loop through layers, calling:
**** state_below = MLP.apply_dropout(state_below)
***** The core of dropout is here. It just masks and scales state_below.
**** state_below = layer.fprop(state_below)
***** fprops (the masked and scaled) input as usual.
*** model.fprop_dropout() zero-masks and scales the inputs, but scaling the learning rate of the prior layer is the responsibility of the user.
** strat 1: leave learning rate change to user (ew)
*** Dropout is a wrapper Node around a Node. No need to know the node's type; it just masks and scales node.output_symbol.
**** InputNode: applies mask, scales output
**** Linear: applies mask, scales output
**** Bias: error or warning
**** AffineTransform: applies mask, scales output
** strat 2: let Dropout be a flag for functions in models.py that create CNN/NNs
*** arg: dropout_include_rates
*** No need for DropoutSgdParameterUpdater for now.
*** Adds dropout nodes after each layer for which dropout_include_rate is not None, and scales that layer's weights (not biases) by p^2

* DONE Check out Fuel as a source of dataset wrappers
** https://github.com/bartvm/fuel
** If unsatisfactory, Implement data.Mnist, test against pylearn2.datasets.Mnist
*** Searches for cached mnist memmaps, creates them if necessary.
* DONE Write mnist.py
* DONE install CUDNN, confirm that Theano's using it
* DONE Implement Conv2D node, Pool2D node
* DONE Write examples/visualize_mnist.py
*** DONE On arrow right or left, shows the trained model the next/previous MNIST test digit.
*** Shows softmax, the argmax of the softmax, the target label, and 10 optimized images
*** Show a softmax below each of the optimized images
**** DONE The i'th image is what you get if you optimize output_softmax[i], starting from the displayed image.
** Debug "optimized" images
*** DONE Try optimizing the cross-entropy of softmax with appropriate one-hot, rather than optimizing one dimension of the softmax.
**** Yeah that looks to be making better progress
*** Try using StopOnStagnation rather than a fixed number of iterations
* DONE write sliding test_pool2d and test_conv2d
** DONE Finish refactoring test_pool2d, confirm it works
** DONE See if you can remove max_pad arg from apply_subwindow_func, and supply just padded_image rather than max_padded_image, where padded_image is padded by exactly actual_pad, which is now renamed "pads".
** Write test_conv2d
*** debug: I think it can't deal with pad > window_size. Try reducing max pad to window_size - 1.
* DONE Write LinearScale(EpochCallback)
** __init__(self, shared_var, final_value, num_epochs)
* DONE Write examples/mnist_fully_connected.py
** DONE We need to make a subclass of Dataset for MNIST, so it knows how to (not) serialize itself. This can be general; call it HdfDataset.
** DONE Stack AffineTransform, Softmax, and CrossEntropy into a fully-connected NN, with layer geometries chosen from pylearn2's tutorials.
** DONE Debug the weight updates shrinking to zero (or being zero most of the time).
*** I fixed it by replacing cost=cross_entropies.sum() with cost=cross_entropies.mean()
** DONE Write save-on-best callback, put it in mnist demo
** DONE Save best model trained on MNIST
** The Pylearn2's mlp demo uses a momentum scale-er that linearly interpolates between two momentum values, rather than linearly shrinks a scale value. Check that the learning rate scale-er does the same, then change LinearlyScalesOverEpochs to do this.
* DONE Write test for limit_param_norms
* DONE Write examples/mnist_conv.py
* DONE Re-check momentum-based updating in pylearn2
** Looked at pylearn2.training_algorithms.learning_rule.Momentum; looked pretty equivalent.
* DONE Check mlp.Linear and mlp.ConvElemwise's _modify_updates to see if anything might be going on there that you haven't done.
** Looked at pylearn2.models.mlp.ConvElemwise._modify_updates(); it just implements norm-capping, which seems equivalent to my limit_param_norms(). Same with mlp.Linear._modify_updates()
* Write data/norb.py
** Write Norb class, override its iterator() method to take kwargs for class-instance -> object ID conversion.
*** Without conversion, label is 5 (9?) dimensional.
**  Write two label-processing nodes:
*** Can't: Theano can't do hash-table lookups. This is only possible if the NORB dataset stores Object IDs, which could then be mapped to Class and Instance IDs by a Node.
*** NorbLabelToClass
*** NorbLabelToInstance
*** NorbLabelToElevation
*** NorbLabelToAzimuth
*** NorbLabelToLighting
*** NorbLabelToHeading
*** NorbLabelToContrast
*** NorbLabelToLuminance
*** NorbLabelToShift
*** 
** write load_norb_image_file(image_hdf_dataset_batch)
** load_norb_label_file(label_hdf_dataset_batch)
** raw_norb_to_hdf(raw_image_files, raw_label_files, hdf_path)
*** Concatenates test & train data into single hdf tensors. Returns hdf tensors, train size.
** load_norb(hdf_path or which_norb=['big, 'small'], which_set=['test, 'train'])
*** These all just call _load_norb_impl(hdf_path)
*** If using which_norb and which_set, these create a hdf file using raw_norb_to_hdf if necessary, then return _load_norb_impl(hdf_path).
** make_norb_instance_dataset(input_hdf, output_hdf, elevation_test_stride, azimuth_test_stride, object_whitelist)
*** load_norb(input_hdf, 'test'), load_norb(input_hdf, 'train'),
*** open tmp_hdf (tmp_hdf) with unified images and labels
*** copy from input_hdf's test and train to the unified tensors of tmp_hdf
*** shuffle the unified tensors of tmp_hdf.
*** Create test_mask by logical_and((unified_labels[:, elevation] % elevation_test_stride) == 0, unified_labels[:, azimuth] % (azimuth_test_stride * 2), unified_labels[:, :2] == object_whitelist[0], unified_labels[:, :2] == object_whitelist[1], ...)
*** open output_hdf(output_hdf), with 'train' and 'test' groups.
*** output_hdf.train.images = tmp_hdf.images[logical_not(test_mask), ...]
*** output_hdf.train.labels = tmp_hdf.labels[logical_not(test_mask), :]
*** output_hdf.test.images = tmp_hdf.images[test_mask, ...]
*** output_hdf.test.labels = tmp_hdf.labels[test_mask, :]
** These instance datasets should now be loadable using load_norb(hdf_path).
** Write a quick examples/browse_norb.py --input hdf_file to step through a norb dataset's images and labels.
* Write examples/train_norb
* Visualize classification + orientation features
* Scale to 6DOF


Debugging test_conv:
* expected_images should not have first row and column filled with zeros.
* aside from that row and column, numbers dont match
** Try filling kernels with all ones, then their convolution will be unique to a particular location, not conv vs cross-correlation.


Todo once test_conv is working:
* re-enable pad sizes bigger than window sizes; see what happens.
* enable pad keywords like 'valid', 'full', and 'same_size'
* Rename InputNode to LeafNode, let params be LeafNodes, then replace LinearColumnNormLimiter and Conv2dFilterNormLimiter with a single class, NormLimiter, which now knows which axes to sum along when computing the norm, because the params have labeled axes (sum over all params other than the designated output axis, labeled in the NormLimiter's constructor.

Debugging pickling Dropout:
* Not even dill can pickle it.
* can't pickle regardless of device=cpu or gpu
* Saving just the SerializeableModel and the validation_loss_logger works fine. Saving the trainer doesn't.
* We're able to save models that include dropout nodes just fine, and then deserialize and use them later.
* RandomState, RandomStreams, and Dropout seem to pickle fine in isolation in ipython.
* It has to be something in the trainer that's not in the model.
** Tried not saving the update_function. Didn't help.
** Maybe it's something in the SgdParamUpdater?
* Theory, debunked: It's most likely a problem with RandomStreams that only gets triggered after it's been called.
** Tried calling function compiled from a simple InputNode-Dropout chain, then pickling the Dropout and the RandomStreams. Both worked fine.
* Theory: It's most likely a problem with Dropout that only gets triggered after a bprop through a random mask.

Thoughts on serialization:
* Either the compiled function is the problem, or the Theano graph is the problem.
** If it's just the function, no big deal at all. Solve in Sgd.__getstate__, __setstate__.
** If the (RandomStream-created) nodes need to not be saved, now we have a problem.
*** We need to rebuild DAG upon deserialization. So save just the Nodes, not their theano symbols.
*** This is doable if all objects only store links to Nodes, not theano symbols, and the DAG is serialized and deserialized with strict bottom-up ordering.
**** SgdUpdater now contains references not to params, but to the Nodes that contain them, and a means to access them from the node. So you'd need to specify both node (e.g. Linear) and field name (e.g. 'params'), so you can say self.param_to_update = node.__dict__[param_field_name]
*** Shared variables need to be owned uniquely, so we know whose job it is to serialize/deserialize it.



* Nodes need to have, in their member variables, the information needed to rebuild themselves from serialization.
* SgdParameterUpdater needs to be able to create its own gradient
** Rewrite it in terms of nodes. We need a GradientNode that takes an X scalar node (axes=(), size=())  and an Y node, and outputs dX/dY, inheriting Y's axes and shape as the output.
** SgdParameterUpdater contains as many Nodes as it has outputs: 
* Ultimately, the cost function should be an all-encompassing Node that takes input and targets, and outputs a scalar (or batch of scalars).
* For the moment, either Dropout is the problem, or some Theano state from the compiled function is. We can refuse to serialize the function, instead re-compiling it upon deserialization.
** Override Sgd.__getstate__() to not save self._update_function, and Sgd.__setstate__() to re-compile update function
** If that doesn't work, Node may need:
*** an abstract build_theano_nodes() method that re-builds the theano graph from input nodes' output_symbols, and returns the node's output_symbol.
*** __getstate__ that leaves out the Theano nodes (saves self._output_symbol as "left unserialized").
*** __setstate__ that leaves out the Theano nodes (asserts that self._output_symbol is "left unserialized").
*** output_symbol is now a property that returns self._output_symbol, unless it's "left unserialized", in which case it constructs it using _build_theano_nodes() if necessary.

Simpler band-aid solution to just save models:
* Pair each model-creating function with a model-saving and model-loading function.
* Or, create a Model class with abstract methods:
** __init__(): the model-creating func
** __getstate__(): the model-saving func
** __setstate__(): the model-loading func
* Abandons re-start-ability (doesn't save training updater state)
* Scales linearly with types of models, brittle to model changes. The serializeable Nodes approach above scales linearly with # of nodes, and can in theory use one unit test for all Nodes' serialization/unserialization.

In-between: serializeable nodes, but forget about serializeable updates for now (you might get that for free)
* Implement not saving Sgd._update_function.
** See if that fixes things.
* If not, implement serializable Nodes
** See if that fixes things.
* Step-by-step:
** Temporarily disable Dropout in mnist demo, confirm that serialization works again.
** checkout master, create new branch serializeable_nodes
** Implement not-saving _update_function in Sgd.
** Commit serializeable_nodes.
** switch back to mnist_demo branch, confirm still working, re-enable Dropout, confirm not working anymore.
** Merge serializeable_nodes into mnist_demo. See if that fixes serializeability.
** If not, switch back to serializeable_nodes, implement serializeable nodes
** Commit serializeable_nodes
** switch back to mnist_demo branch, merge serializeable_nodes into mnist_demo, se if that fixes serializeability.


* NORB brainstorm
** Need support for:
*** variable-length label vectors (5 vs 11)
*** label vector conversions are generally non-differentiable. Don't make them Theano nodes. Make them iterators.
*** How to specify iterator label format?
**** Just provide a function that takes a label and outputs a value.
**** Independently, we also need to provide an implementation of make_input_nodes that returns InputNodes with the approprate DenseFormats.
***** One more reason to move make_input_nodes to iterator.
**** One solution: just subclass Norb and override .iterator() and .make_input_nodes
**** Another solution: make it the iterator's job to impelent make_input_nodes
***** Write ConvertingIterator, which wraps an existing iterator and converts its output (though its implementation of _next() and _make_input_nodes()).
***** This is better because it sepearates the concerns of iteration order vs format. 


* TODO Friday:
** DONE Get mnist working
** DONE give Dataset a __getitem__ method that can take ints or slices.
** DONE In Hdf5Dataset, override __getitem__ to also take strings.
** change norb.py to use new Hdf5Dataset[Slice].
** Finish instance dataset making for stereo
** add support for mono
** port images & labels memmap to mono-NORB converter script
*** Don't forget jitter for x & y position.
** Create instance dataset for 6-objects dataset
** Write script for classifying 6-objects dataset.
** Write script for classifying & camera angle regression
** Add position regression (i.e. position offset from center).
