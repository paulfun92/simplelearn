* DONE Get linear_regression.py to live-plot values of interest
** Average training, testing costs
* DONE Write unit test for ReLU node.
* DONE Write unit test for Softmax node, against pylearn2's.
* DONE Implement CrossEntropy node, unit-test against pylearn2's.
* DONE Implement Dropout
* DONE Check out Fuel as a source of dataset wrappers
** https://github.com/bartvm/fuel
** If unsatisfactory, Implement data.Mnist, test against pylearn2.datasets.Mnist
*** Searches for cached mnist memmaps, creates them if necessary.
* DONE Write mnist.py
* Write examples/mnist_fully_connected.py
** DONE We need to make a subclass of Dataset for MNIST, so it knows how to (not) serialize itself. This can be general; call it HdfDataset.
** DONE Stack AffineTransform, Softmax, and CrossEntropy into a fully-connected NN, with layer geometries chosen from pylearn2's tutorials.
** DONE Debug the weight updates shrinking to zero (or being zero most of the time).
*** I fixed it by replacing cost=cross_entropies.sum() with cost=cross_entropies.mean()
** Implement model serialization
** Save model trained on MNIST
** Write demo that:
*** On arrow right or left, shows the trained model the next/previous MNIST test digit.
*** Shows softmax, the argmax of the softmax, the target label, and 10 images:
**** The i'th image is what you get if you optimize output_softmax[i], starting from the displayed image.
* Implement Conv2D node, test against pylearn2's.
* Write examples/mnist_conv.py
* Write examples/cifar10.py
* Write data/norb.py
* Replicate classification success on big NORB
* Replicate classification + orientation success.
* Write classification demo


Dropout brainstorm:
* Requirements:
** For a given unit, include its inputs with probability p (aka zero out with prob. 1-p).
** Scale the inputs by 1/p to compensate for lowered output vector magnitude.
** If the inputs are the output of some parameterized function, the parameters' learning rates should be scaled by p^2 (lowered), to compensate for heightend gradient due to scaling by 1/p above.
* What pylearn2 does
** dropout is a cost object, with its own rng. Evaluating it causes it to call mlp.dropout_fprop() instead of mlp.fprop().
** This in turn causes it to loop through layers, calling:
*** state_below = MLP.apply_dropout(state_below)
**** The core of dropout is here. It just masks and scales state_below.
*** state_below = layer.fprop(state_below)
**** fprops (the masked and scaled) input as usual.
** model.fprop_dropout() zero-masks and scales the inputs, but scaling the learning rate of the prior layer is the responsibility of the user.
* strat 1: leave learning rate change to user (ew)
** Dropout is a wrapper Node around a Node. No need to know the node's type; it just masks and scales node.output_symbol.
*** InputNode: applies mask, scales output
*** Linear: applies mask, scales output
*** Bias: error or warning
*** AffineTransform: applies mask, scales output
* strat 2: let Dropout be a flag for functions in models.py that create CNN/NNs
** arg: dropout_include_rates
** No need for DropoutSgdParameterUpdater for now.
** Adds dropout nodes after each layer for which dropout_include_rate is not None, and scales that layer's weights (not biases) by p^2
