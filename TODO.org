Next: flesh out sgd to contain a list of updaters, and build the theano function (with updates) on train().

Sgd._function = theano.function(inputs=input_nodes' inputs,
                                outputs=monitored_values,
                                updates=updates)
* inputs: maybe taken from dataset.make_input_nodes
monitored_values: a list of expressions that are the actual outputs of the theano.function.
updates: a dictionary of values and their replacements, that are the "updates" arguments to theano.function

* Get sgd_demo.py working
** DONE Finish testing Dataset interface
** Create monitor that monitors cost and raises a StopTraining exception if it fails to increase for N epochs.
*** NEXT: class: StopsOnStagnation
**** __call__(self, cost):
***** if cost + self._margin < prev_cost, self._counter = 0 else self._counter += 1
***** if self._counter >= self._max_stagnant_epochs: raise StopTraining("%s stagnant for %d epochs." % (self._cost_name, self._max_stagnant_epochs))
*** DONE class: ComputesAverageOverEpoch
**** __init__ args: (cost)function, validation_iter, callbacks
**** __call__(self): compute average value of self._function(*self._data_iterator.next()) over an epoch, pass result to each of self._callbacks
*** Give ComputesAverageOverEpoch(cost_function, dummy_dataset.iterator(), (StopsOnStagnation('cost'), lambda x : validation_costs.append(x))) to Sgd as epoch callback.
**** __iter__ args: cost func, validation_iter, margin
**** __call__(self): sum total cost until validation_iter's epoch increments, divide by # of samples (not batches), see if it decreased more than margin relative to previous avg sum, if so, set counter to 0; if not, increment counter.
*** Test by feeding it an L2-norm cost, and a dataset that increments an epoch after every sample, where the sample is just a random vector whose magnitude shrinks linearly with epochs, then at some point stops changing at all. Should quit N points into the zero-slope line.
** Finish sgd demo
*** Create dummy dataset that repeatedly yields the input shared variable's numeric value for each epoch.
*** Use Sgd with DummyDataset, QuadraticCost, CostMonitor


Brainstorm:

ComputeAverageOverEpoch:
* Would like to make it work both for training and testing data
** Option: use a different monitor for computing the average of an epoch of training data.
*** An updater can monitor the training cost. (how does Pylearn2 do it?
** (bad) Option: give the training set monitor a fresh iterator, just like the testing set
*** Bad: for random iterators, there's no guarantee that this measures the training set error over the same data as what the model was trained on.
** (bad) Option: Overload ComputeAverageOverEpoch.__call__()
*** If it's called without arguments, compute average of f(x) over iterator passed in through constructor.
*** If it's called with (input_tuple), then compute f(*input_tuple)?
*** Messy as hell: assumes that f is a function of input_tuple


* (For now, let this be.) DataIterator should have the Nodes, so it can be the sole interface for data consumers.
** The name DataIterator in this case bothers me.
** This will be necessary anyway if (if) the iterator ever is to yield a different number of variables than the original dataset. For example, an iterator that yields left and right stereo images as separate variables.
*** This won't be necessary in the forseeable future. If node L operates on left images and node R operates on right images, they can both take a stereo image as input, and just use their respective parts of it.
** For now, let it be.
* DONE Epoch events should be fired after the last datum of the epoch is yielded, not on the first datum of the next epoch.
** Can't do this by yielding epoch numbers!
** This could eliminate the present need for batch() and even epoch() methods.
** Alternatives to epoch numbers:
*** boolean DataIterator.epoch_done() method
**** In trainer, get datum, if iterator.epoch_done(), call epoch callbacks.
*** iterator.next() yields data, epoch_done, where data is always a tuple.
**** Similar to above, but no need for creating a named iterator variable when looping
*** register epoch callbacks with iterator itself
**** This would necessarily trigger during the next() call that yields the final datum in an epoch
**** This is bad: the callbacks should happen after processing the final datum in train().
