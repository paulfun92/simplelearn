* Get sgd_demo.py working
** DONE Finish testing Dataset interface
** Create monitor that monitors cost and raises a StopTraining exception if it fails to increase for N epochs.
*** class: ComputesAverageOverEpoch
**** __init__ args: (cost)function, validation_iter, callbacks
**** __call__(self): compute average value of self._function(*self._data_iterator.next()) over an epoch, pass result to each of self._callbacks
*** class: StopsOnStagnation
**** __call__(self, cost):
***** if cost + self._margin < prev_cost, self._counter = 0 else self._counter += 1
***** if self._counter >= self._max_stagnant_epochs: raise StopTraining("%s stagnant for %d epochs." % (self._cost_name, self._max_stagnant_epochs))
*** Give ComputesAverageOverEpoch(cost_function, dummy_dataset.iterator(), (StopsOnStagnation('cost'), lambda x : validation_costs.append(x))) to Sgd as epoch callback.
**** __iter__ args: cost func, validation_iter, margin
**** __call__(self): sum total cost until validation_iter's epoch increments, divide by # of samples (not batches), see if it decreased more than margin relative to previous avg sum, if so, set counter to 0; if not, increment counter.
*** Test by feeding it an L2-norm cost, and a dataset that increments an epoch after every sample, where the sample is just a random vector whose magnitude shrinks linearly with epochs, then at some point stops changing at all. Should quit N points into the zero-slope line.
** Finish sgd demo
*** Create dummy dataset that repeatedly yields the input shared variable's numeric value for each epoch.
*** Use Sgd with DummyDataset, QuadraticCost, CostMonitor


Brainstorm:

* Epoch events should be fired after the last datum of the epoch is yielded, not on the first datum of the next epoch.
** Can't do this by yielding epoch numbers!
** This could eliminate the present need for batch() and even epoch() methods.
** Alternatives to epoch numbers:
*** boolean DataIterator.epoch_done() method
**** In trainer, get datum, if iterator.epoch_done(), call epoch callbacks.
*** iterator.next() yields data, epoch_done, where data is always a tuple.
**** Similar to above, but no need for creating a named iterator variable when looping
*** register epoch callbacks with iterator itself
**** This would necessarily trigger during the next() call that yields the final datum in an epoch
**** This is bad: the callbacks should happen after processing the final datum in train().
* DataIterator should have the Nodes, so it can be the sole interface for data consumers.
** The name DataIterator in this case bothers me.
** This will be necessary anyway if (if) the iterator ever is to yield a different number of variables than the original dataset. For example, an iterator that yields left and right stereo images as separate variables.
*** This won't be necessary in the forseeable future. If node L operates on left images and node R operates on right images, they can both take a stereo image as input, and just use their respective parts of it.
** For now, let it be.
